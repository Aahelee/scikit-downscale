{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-downscale: an open source Python package for scalable climate downscaling\n",
    "\n",
    "Joseph Hamman (jhamman@ucar.edu) and Julia Kent (jkent@ucar.edu)\n",
    "\n",
    "NCAR, United States of America\n",
    "\n",
    "ECAHM 2020 ID: 143\n",
    "\n",
    "Climate data from Earth System Models are increasingly being used to study the impacts of climate change on a broad range of biogeophysical (forest fires, fisheries, etc.) and human systems (reservoir operations, urban heat waves, etc.). Before this data can be used to study many of these systems, post-processing steps commonly referred to as bias correction and statistical downscaling must be performed. “Bias correction” is used to correct persistent biases in climate model output and “statistical downscaling” is used to increase the spatiotemporal resolution of the model output (i.e. 1 deg to 1/16th deg grid boxes). For our purposes, we’ll refer to both parts as “downscaling”.\n",
    "\n",
    "In the past few decades, the applications community has developed a plethora of downscaling methods. Many of these methods are ad-hoc collections of post processing routines while others target very specific applications. The proliferation of downscaling methods has left the climate applications community with an overwhelming body of research to sort through without much in the form of synthesis guilding method selection or applicability.\n",
    "\n",
    "Motivated by the pressing socio-environmental challenges of climate change – and with the learnings from previous downscaling efforts in mind – we have begun working on a community-centered open framework for climate downscaling: scikit-downscale. We believe that the community will benefit from the presence of a well-designed open source downscaling toolbox with standard interfaces alongside a repository of benchmark data to test and evaluate new and existing downscaling methods.\n",
    "\n",
    "In this notebook, we provide an overview of the scikit-downscale project, detailing how it can be used to downscale a range of surface climate variables such as air temperature and precipitation. We also highlight how scikit-downscale framework is being used to compare exisiting methods and how it can be extended to support the development of new downscaling methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Insert some sort of figure here, probably showing a “typical” workflow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-downscale\n",
    "Scikit-downscale is a new Python project we have been compiling over the past few months. In it, we have been building a collection of existing downscaling methods within a common framework. Key features of Scikit-downscale are:\n",
    "\n",
    "A high-level interface modeled after the popular fit / precict pattern found in many machine learning packages (Scikit-learn, Tensorflow, etc.),\n",
    "Uses Xarray data structures and utilities for handling multi-dimensional datasets and parrlelization,\n",
    "Common interface for pointwise and spatial (or global) downscaling models, and\n",
    "Extensible, allowing the creation of new downscaling methods through composition.\n",
    "Below is an example implementation of a Scikit-downscale workflow that uses the BCSD method:\n",
    "\n",
    "from skdownscale.pointwise_models import PointWiseDownscaler\n",
    "from skdownscale.models.bcsd import BCSDTemperature\n",
    "\n",
    "```python\n",
    "# da_temp_train: xarray.DataArray (monthly)\n",
    "# da_temp_obs: xarray.DataArray (monthly)\n",
    "# da_temp_obs_daily: xarray.DataArray (daily)\n",
    "# da_temp_predict: xarray.DataArray (monthly)\n",
    "\n",
    "# create a model\n",
    "bcsd_model = PointWiseDownscaler(BCSDTemperature(), dim='time')\n",
    "\n",
    "# train the model\n",
    "bcsd_model.train(da_temp_train, da_temp_obs)\n",
    "\n",
    "# predict with the model  (downscaled_temp: xr.DataArray)\n",
    "downscaled_temp = bcsd_model.predict(da_temp_predict)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointwise Models\n",
    "We define pointwise methods are those that only use local information during the downscaling process. They can be often represented as a linear model and applied repetively across the entire study domain. Examples of existing pointwise methods are:\n",
    "\n",
    "- BCSD_[Temperature, Precipitation]: Wood et al 2002\n",
    "- ARRM: Stoner et al 2012\n",
    "- (Hybrid) Delta Method\n",
    "- GARD: https://github.com/NCAR/GARD\n",
    "\n",
    "Because pointwise methods can be written as a stand-alone linear model, Scikit-downscale implements these models as a Scikit-learn LinearModel or Pipeline. By building directly on Scikit-learn, we inherit a well defined model API and the ability to interoperate with a robust ecosystem utilities for model evaluation and optimization (e.g. grid-search). Perhaps more importantly, this structure also allows us to compare methods at a high-level of granularity (single spatial point) before deploying them on large domain problems.\n",
    "\n",
    "In the example above, we demonstrated the use of the PointWiseDownscaler. We use this class to wrap a pointwise models allowing training and prediction with multidimensional Xarray objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Models\n",
    "Spatial models is a second class of downscaling methods that use information from the full study domain to form relationships between observations and ESM data. Scikit-downscale implements these models as as SpatialDownscaler. Beyond providing fit and predict methods that accept Xarray objects, the internal layout of these methods is intentionally unspecified. We are currently working on wrapping a few popular spatial downscaling models such as:\n",
    "\n",
    "- MACA\n",
    "- LOCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Applications\n",
    "Its likely that one of the reasons we haven’t seen strong consensus develop around particularl downscaling methodologies is the abscense of widely available benchamrk applications to test methods against eachother. We haven’t solved this problem but we are motivated to work accross the community to develop a new benchmark applications.\n",
    "\n",
    "Data\n",
    "\n",
    "Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #1 Gard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from xsd.pointwise_models import PureAnalog, AnalogRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a small dataset for training\n",
    "training = xr.open_zarr('../data/downscale_test_data.zarr.zip', group='training')\n",
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a small dataset of observations (targets)\n",
    "targets = xr.open_zarr('../data/downscale_test_data.zarr.zip', group='targets')\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 1 point of training data for precipitation and temperature \n",
    "X_temp = training.isel(point=0).to_dataframe()[['T2max']] - 273.13\n",
    "X_pcp = training.isel(point=0).to_dataframe()[['PREC_TOT']] * 24\n",
    "display(X_temp.head(), X_pcp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 1 point of target data for precipitation and temperature \n",
    "y_temp = targets.isel(point=0).to_dataframe()[['Tmax']]\n",
    "y_pcp = targets.isel(point=0).to_dataframe()[['Prec']]\n",
    "display(y_temp.head(), y_pcp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/predict using the PureAnalog class\n",
    "for kind in ['best_analog', 'sample_analogs', 'weight_analogs', 'mean_analogs']:\n",
    "    pure_analog = PureAnalog(kind=kind, n_analogs=10)\n",
    "    pure_analog.fit(X_temp[:1000], y_temp[:1000])\n",
    "    out = pure_analog.predict(X_temp[1000:])\n",
    "\n",
    "    plt.plot(out[:300], label=kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/predict using the AnalogRegression class\n",
    "analog_reg = AnalogRegression(n_analogs=100)\n",
    "analog_reg.fit(X_temp[:1000], y_temp[:1000])\n",
    "out = analog_reg.predict(X_temp[1000:])\n",
    "plt.plot(out[:300], label='AnalogRegression')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #2 bcsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from xsd.pointwise_models import BcsdTemperature, BcsdPrecipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities for plotting cdfs\n",
    "def plot_cdf(ax=None, **kwargs):\n",
    "    if ax:\n",
    "        plt.sca(ax)\n",
    "    else:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    for label, X in kwargs.items():\n",
    "        vals = np.sort(X, axis=0)\n",
    "        pp = scipy.stats.mstats.plotting_positions(vals)  \n",
    "        ax.plot(pp, vals, label=label)\n",
    "    ax.legend()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_cdf_by_month(ax=None, **kwargs):\n",
    "    fig, axes = plt.subplots(4, 3, sharex=True, sharey=False, figsize=(12, 8))\n",
    "    \n",
    "    for label, X in kwargs.items():\n",
    "        for month, ax in zip(range(1, 13), axes.flat):\n",
    "            \n",
    "            vals = np.sort(X[X.index.month == month], axis=0)\n",
    "            pp = scipy.stats.mstats.plotting_positions(vals)  \n",
    "            ax.plot(pp, vals, label=label)\n",
    "            ax.set_title(month)\n",
    "    ax.legend()\n",
    "    return ax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a small dataset for training\n",
    "training = xr.open_zarr('../data/downscale_test_data.zarr.zip', group='training')\n",
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a small dataset of observations (targets)\n",
    "targets = xr.open_zarr('../data/downscale_test_data.zarr.zip', group='targets')\n",
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 1 point of training data for precipitation and temperature \n",
    "X_temp = training.isel(point=0).to_dataframe()[['T2max']].resample('MS').mean() - 273.13\n",
    "X_pcp = training.isel(point=0).to_dataframe()[['PREC_TOT']].resample('MS').sum() * 24\n",
    "display(X_temp.head(), X_pcp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract 1 point of target data for precipitation and temperature \n",
    "y_temp = targets.isel(point=0).to_dataframe()[['Tmax']].resample('MS').mean()\n",
    "y_pcp = targets.isel(point=0).to_dataframe()[['Prec']].resample('MS').sum()\n",
    "display(y_temp.head(), y_pcp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/predict the BCSD Temperature model\n",
    "bcsd_temp = BcsdTemperature()\n",
    "bcsd_temp.fit(X_temp, y_temp)\n",
    "out = bcsd_temp.predict(X_temp) + X_temp\n",
    "plot_cdf(X=X_temp, y=y_temp, out=out)\n",
    "out.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf_by_month(X=X_temp, y=y_temp, out=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit/predict the BCSD Precipitation model\n",
    "bcsd_pcp = BcsdPrecipitation()\n",
    "bcsd_pcp.fit(X_pcp, y_pcp)\n",
    "out = bcsd_pcp.predict(X_pcp) * X_pcp  \n",
    "plot_cdf(X=X_pcp, y=y_pcp, out=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cdf_by_month(X=X_pcp, y=y_pcp, out=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #3 zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from xsd.pointwise_models import ZScoreRegresssor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a small dataset\n",
    "data = xr.open_zarr('../data/downscale_test_data.zarr.zip', group='UAS')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select one point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align time\n",
    "def _cfnoleap_to_datetime(da):\n",
    "    datetimeindex = da.indexes['time'].to_datetimeindex()\n",
    "    ds = da#.to_dataset()\n",
    "    ds['time_dt']= ('time', datetimeindex)\n",
    "    ds = ds.swap_dims({'time': 'time_dt'})\n",
    "    assert len(da.time) == len(ds.time_dt)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regroup models by time\n",
    "def _regroup_models_bytime(ds_meas, ds_hist_dt, ds_rcp_dt):\n",
    "    t0_meas = ds_meas.time[0]\n",
    "    tn_meas = ds_meas.time[-1]\n",
    "    t0_fut = tn_meas.values + np.timedelta64(1, 'D')\n",
    "    \n",
    "    ds_past = ds_hist_dt.sel(time_dt = slice(t0_meas, tn_meas))\n",
    "    ds_past = ds_past.swap_dims({'time_dt':'time'})\n",
    "    \n",
    "    ds_fut_pt1 = ds_hist_dt.sel(time_dt = slice(t0_fut,None))\n",
    "    ds_fut = xr.concat([ds_fut_pt1[var], ds_rcp_dt[var]], 'time_dt')\n",
    "    ds_fut = ds_fut.swap_dims({'time_dt':'time'})\n",
    "    return ds_past, ds_fut\n",
    "\n",
    "ds_past, ds_fut = _regroup_models_bytime(ds_meas_noleap, ds_hist_dt, ds_rcp85_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias correction using ZScoreRegresssor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the correction\n",
    "def gaus(mean, std, doy):\n",
    "    a = mean.sel(day=doy)\n",
    "    mu = a.isel(lon = 0, lat = 0)\n",
    "\n",
    "    b =std.sel(day=doy)\n",
    "    sigma = b.isel(lon = 0, lat = 0)\n",
    "\n",
    "    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "    y = stats.norm.pdf(x, mu, sigma)\n",
    "    return x, y\n",
    "\n",
    "fut_typ_mean, fut_typ_std, fut_typ_zscore = _calc_stats(ds_fut, window_width)\n",
    "fut_typ_mean_bc = fut_typ_mean + shift\n",
    "fut_typ_std_bc = fut_typ_std * scale\n",
    "\n",
    "doy=20\n",
    "plt.figure()\n",
    "x,y = gaus(hist_mean[var], hist_std[var], doy)\n",
    "plt.plot(x, y, 'orange', label = 'historical model')\n",
    "x,y = gaus(meas_mean[var], meas_std[var], doy)\n",
    "plt.plot(x, y, 'red', label = 'measured')\n",
    "x,y = gaus(fut_typ_mean, fut_typ_std, doy)\n",
    "plt.plot(x, y, 'blue', label = 'raw future model')\n",
    "x,y = gaus(fut_typ_mean_bc[var], fut_typ_std_bc[var], doy)\n",
    "plt.plot(x, y, 'green', label = 'corrected future model')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call for Participation\n",
    "This effort is just getting started. With the recent release of CMIP6, we expect a surge of interest in downscaled climate data. There are clear opportunities for involvement from climate impacts practicioneers, computer scientists with an interest in machine learning for climate applications, and climate scientists alike. Please reach out if you are interested in participating in any way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
