{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-downscale: an open source Python package for scalable climate downscaling\n",
    "\n",
    "Joseph Hamman (jhamman@ucar.edu) and Julia Kent (jkent@ucar.edu)\n",
    "\n",
    "NCAR, Boulder, CO, USA\n",
    "\n",
    "ECAHM 2020 ID: 143\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Climate data from Earth System Models (ESMs) are increasingly being used to study the impacts of climate change on a broad range of biogeophysical systems (forest fire, flood, fisheries, etc.) and human systems (water resources, power grids, etc.). Before this data can be used to study many of these systems, post-processing steps commonly referred to as bias correction and statistical downscaling must be performed. “Bias correction” is used to correct persistent biases in climate model output and “statistical downscaling” is used to increase the spatiotemporal resolution of the model output (i.e. from 1 deg to 1/16th deg grid boxes). For our purposes, we’ll refer to both parts as “downscaling”.\n",
    "\n",
    "In the past few decades, the applications community has developed a plethora of downscaling methods. Many of these methods are ad-hoc collections of processing routines while others target very specific applications. The proliferation of downscaling methods has left the climate applications community with an overwhelming body of research to sort through without much in the form of synthesis guilding method selection or applicability.\n",
    "\n",
    "Motivated by the pressing socio-environmental challenges of climate change – and with the learnings from previous downscaling efforts (e.g. Gutmann et al. 2014, Lanzante et al. 2018) in mind – we have begun working on a community-centered open framework for climate downscaling: [scikit-downscale](https://scikit-downscale.readthedocs.io/en/latest/). We believe that the community will benefit from the presence of a well-designed open source downscaling toolbox with standard interfaces alongside a repository of benchmark data to test and evaluate new and existing downscaling methods.\n",
    "\n",
    "In this notebook, we provide an overview of the scikit-downscale project, detailing how it can be used to downscale a range of surface climate variables such as surface air temperature and precipitation. We also highlight how scikit-downscale framework is being used to compare exisiting methods and how it can be extended to support the development of new downscaling methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scikit-downscale\n",
    "Scikit-downscale is a new open source Python project. Within Scikit-downscale, we are been building a collection of new and existing downscaling methods within a common framework. Key features of Scikit-downscale are:\n",
    "\n",
    "- A high-level interface modeled after the popular _fit_ / _predict_ pattern found in many machine learning packages ([Scikit-learn](https://scikit-learn.org/stable/index.html), [Tensorflow](https://www.tensorflow.org/guide/keras), etc.),\n",
    "- Uses [Xarray](http://xarray.pydata.org/en/stable/) and [Pandas](https://pandas.pydata.org/) data structures and utilities for handling of labeled datasets,\n",
    "- Utilities for automatic parallelization of pointwisde downscaling models,\n",
    "- Common interface for pointwise and spatial (or global) downscaling models, and\n",
    "- Extensible, allowing the creation of new downscaling methods through composition.\n",
    "\n",
    "Scikit-downscale's source code is available on [GitHub](https://github.com/jhamman/scikit-downscale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Pointwise Models\n",
    "We define pointwise methods as those that only use local information during the downscaling process. They can be often represented as a linear model and fit independently across the entire study domain. Examples of existing pointwise methods are:\n",
    "\n",
    "- BCSD_[Temperature, Precipitation]: Wood et al. (2004)\n",
    "- ARRM: Stoner et al. (2012)\n",
    "- (Hybrid) Delta Method (e.g. Hamlet et al. (2010)\n",
    "- [GARD](https://github.com/NCAR/GARD): Gutmann et al (in prep). \n",
    "\n",
    "Because pointwise methods can be written as a stand-alone linear model, Scikit-downscale implements these models as a Scikit-learn [LinearModel](https://scikit-learn.org/stable/modules/linear_model.html) or [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). By building directly on Scikit-learn, we inherit a well defined model API and the ability to interoperate with a robust ecosystem utilities for model evaluation and optimization (e.g. grid-search). Perhaps more importantly, this structure also allows us to compare methods at a high-level of granularity (single spatial point) before deploying them on large domain problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "***Begin interactive demo***\n",
    "\n",
    "From here forward in this notebook, we'll jump back and forth between Python and text cells to describe how scikit-downscale works.\n",
    "\n",
    "This first cell just imports some libraries and get's things setup for our analysis to come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils import get_sample_data\n",
    "\n",
    "sns.set(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've imported a few libraries, let's open a sample dataset from a single point in North America. We'll use this data to explore Scikit-downscale and its existing functionality. You'll notice there are two groups of data, `training` and `targets`. We will mostly use the `tmax` variable (daily maximum surface air temperature) going forward. With a small amount of effort, an interested reader could swap `tmax` for `pcp` and test these methods on precipitation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample data\n",
    "training = get_sample_data('training')\n",
    "targets = get_sample_data('targets')\n",
    "\n",
    "# print a table of the training/targets data\n",
    "display(pd.concat({'training': training, 'targets': targets}, axis=1))\n",
    "\n",
    "# make a plot of the temperature and precipitation data\n",
    "fig, axes = plt.subplots(ncols=1, nrows=2, figsize=(8, 6), sharex=True)\n",
    "time_slice = slice('1990-01-01', '1990-12-31')\n",
    "\n",
    "# plot-temperature\n",
    "training[time_slice]['tmax'].plot(ax=axes[0], label='training')\n",
    "targets[time_slice]['tmax'].plot(ax=axes[0], label='targets')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylabel('Temperature [C]')\n",
    "\n",
    "# plot-precipitation\n",
    "training[time_slice]['pcp'].plot(ax=axes[1])\n",
    "targets[time_slice]['pcp'].plot(ax=axes[1])\n",
    "_ = axes[1].set_ylabel('Precipitation [mm/day]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Models as cattle, not pets\n",
    "\n",
    "As we mentioned above, Scikit-downscale utilizes a similiar API to that of Scikit-learn for its pointwise models. This means we can build collections of models that may be quite different internally, but operate the same at the API level. This is perhaps the most important feature of Scikit-downscale, the ability to test and compare arbitrary combinations of models under a common interface. This allows us to try many combinations of models and parameters, choosing only the best combinations.\n",
    "\n",
    "In the cell below, we'll create nine different downscaling models, some from Scikit-downscale and some from Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from skdownscale.pointwise_models import PureAnalog, AnalogRegression\n",
    "from skdownscale.pointwise_models import BcsdTemperature, BcsdPrecipitation\n",
    "\n",
    "\n",
    "models = {\n",
    "    'GARD: PureAnalog-best-1': PureAnalog(kind='best_analog', n_analogs=1),\n",
    "    'GARD: PureAnalog-sample-10': PureAnalog(kind='sample_analogs', n_analogs=10),\n",
    "    'GARD: PureAnalog-weight-10': PureAnalog(kind='weight_analogs', n_analogs=10),\n",
    "    'GARD: PureAnalog-weight-100': PureAnalog(kind='weight_analogs', n_analogs=100),\n",
    "    'GARD: PureAnalog-mean-10': PureAnalog(kind='mean_analogs', n_analogs=10),\n",
    "    'GARD: AnalogRegression-100': AnalogRegression(n_analogs=100),\n",
    "    'GARD: LinearRegression': LinearRegression(),\n",
    "    'BCSD: BcsdTemperature': BcsdTemperature(return_anoms=False),\n",
    "    'Sklearn: RandomForestRegressor': RandomForestRegressor(random_state=0)\n",
    "}\n",
    "\n",
    "train_slice = slice('1980-01-01', '1989-12-31')\n",
    "predict_slice = slice('1990-01-01', '1999-12-31')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created a collection of models, we want to train the models on the same input data. We do this by looping through our dictionary of models and calling the `fit` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract training / prediction data\n",
    "X_train = training[['tmax']][train_slice]\n",
    "y_train = targets[['tmax']][train_slice]\n",
    "X_predict = training[['tmax']][predict_slice]\n",
    "\n",
    "# Fit all models\n",
    "for key, model in models.items():\n",
    "    model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like that, we fit nine downscaling models. Now we want to use those models to downscale/bias-correct our data. For the sake of easy comparison, we'll use a different part of the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store predicted results in this dataframe\n",
    "predict_df = pd.DataFrame(index = X_predict.index)\n",
    "\n",
    "for key, model in models.items():\n",
    "    predict_df[key] = model.predict(X_predict)\n",
    "\n",
    "# show a table of the predicted data\n",
    "display(predict_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do some sample analysis on our predicted data. First, we'll look at a timeseries of all the downscaled timeseries for the first year of the prediction period. In the figure below, the `target` (truth) data is shown in black, the original (pre-correction) data is shown in grey, and each of the downscaled data timeseries is shown in a different color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 3.5))\n",
    "targets['tmax'][time_slice].plot(ax=ax, label='target', c='k', lw=1, alpha=0.75, legend=True, zorder=10)\n",
    "X_predict['tmax'][time_slice].plot(label='original', c='grey', ax=ax, alpha=0.75, legend=True)\n",
    "predict_df[time_slice].plot(ax=ax, lw=0.75)\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "_ = ax.set_ylabel('Temperature [C]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, its difficult to tell which of the nine downscaling methods performed best from our plot above. We may want to evaluate our predictions using a standard statistical score, such as $r^2$. Those results are easily computed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate r2\n",
    "score = (predict_df.corrwith(targets.tmax[predict_slice]) **2).sort_values().to_frame('r2_score')\n",
    "display(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our downscaling methods seem to be doing fairly well. The timeseries and statistics above shows that all our methods are producing generally resonable results. However, we are often interested in how our models do at predicting extreme events. We can quickly look into those aspects of our results using the `qq` plots below. There you'll see that the models diverge in some interesting ways. For example, while the `LinearRegression` method has the highest $r^2$ score, it seems to have trouble capturing extreme heat events. Whereas many of the analog methods, as well as the `RandomForestRegressor`, perform much better on the tails of the distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prob_plots\n",
    "\n",
    "fig = prob_plots(X_predict, targets['tmax'], predict_df[score.index.values], shape=(3, 3), figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we've shown how easy it is to fit, predict, and evaluate scikit-downscale models. The seamless interoperability of these models clearly facilitates a workflow that enables a deeper level of model evaluation that is otherwise possible in the downscaling world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Tailor-made methods in a common framework\n",
    "\n",
    "In the section above, we showed how it is possible to use scikit-downscale to bias-correct a timeseries of daily maximum air temperature using an arbitrary collection of linear models. Some of those models were general machine learning methods (e.g. `LinearRegression` or `RandomForestRegressor`) while others were tailor-made methods developed specifically for downscaling (e.g. `BCSDTemperature`). In this section, we walk through how new pointwise methods can be added to the scikit-downscale framework, highlighting the Z-Score method along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Z-Score Method\n",
    "\n",
    "Z-Score bias correction is a good technique for target variables with Gaussian probability distributions, such as zonal wind speed.\n",
    "\n",
    "In essence the technique:\n",
    "\n",
    "1. Finds the mean  \n",
    "$$\\overline{x} = \\sum_{i=0}^N \\frac{x_i}{N}$$ \n",
    "and standard deviation \n",
    "$$\\sigma = \\sqrt{\\frac{\\sum_{i=0}^N |x_i - \\overline{x}|^2}{N-1}}$$ \n",
    "of target (measured) data and training (historical modeled) data. \n",
    "\n",
    "2. Compares the difference between the statistical values to produce a shift \n",
    "$$shift = \\overline{x_{target}} - \\overline{x_{training}}$$ \n",
    "and scale parameter \n",
    "$$scale = \\sigma_{target} \\div \\sigma_{training}$$ \n",
    "\n",
    "3. Applies these paramaters to the future model data to be corrected to get a new mean\n",
    "$$\\overline{x_{corrected}} = \\overline{x_{future}} + shift$$\n",
    "and new standard deviation\n",
    "$$\\sigma_{corrected} = \\sigma_{future} \\times scale$$\n",
    "\n",
    "4. Calculates the corrected values\n",
    "$$x_{corrected_{i}} = z_i \\times \\sigma_{corrected} + \\overline{x_{corrected}}$$\n",
    "from the future model's z-score values\n",
    "$$z_i = \\frac{x_i-\\overline{x}}{\\sigma}$$\n",
    "\n",
    "In practice, if the wind was on average 3 m/s faster on the first of July in the models compared to the measurements, we would adjust the modeled data for all July 1sts in the future modeled dataset to be 3 m/s faster. And similarly for scaling the standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Building the ZScoreRegressor Class\n",
    "\n",
    "Scikit-downscale's pointwise all implement Scikit-learn's `fit`/`predict` API. Each new downscaler must implement a minimum of three class methods: `__init__`, `fit`, `predict`. \n",
    "\n",
    "```python\n",
    "class AbstractDownscaler(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        ...\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        ...\n",
    "        return self\n",
    "    \n",
    "    def predict(X):\n",
    "        ...\n",
    "        return y_hat\n",
    "```\n",
    "\n",
    "Ommitting some of the complexity in the full implementation (which can be found in the [full implementation on GitHub](https://github.com/jhamman/scikit-downscale/blob/master/skdownscale/pointwise_models/zscore.py)), we demonstrate how the `ZScoreRegressor` was built:\n",
    "\n",
    "First, we define our `__init__` method, allowing users to specify specific options (in this case `window_width`):\n",
    "```python\n",
    "class ZScoreRegressor(object):\n",
    "    \n",
    "    def __init__(self, window_width=31):\n",
    "        self.window_width = window_width\n",
    "```\n",
    "\n",
    "Next, we define our `fit` method, \n",
    "\n",
    "```python\n",
    "    def fit(self, X, y):\n",
    "        X_mean, X_std = _calc_stats(X.squeeze(), self.window_width)\n",
    "        y_mean, y_std = _calc_stats(y.squeeze(), self.window_width)\n",
    "        \n",
    "        self.stats_dict_ = {\n",
    "            \"X_mean\": X_mean,\n",
    "            \"X_std\": X_std,\n",
    "            \"y_mean\": y_mean,\n",
    "            \"y_std\": y_std,\n",
    "        }\n",
    "\n",
    "        shift, scale = _get_params(X_mean, X_std, y_mean, y_std)\n",
    "\n",
    "        self.shift_ = shift\n",
    "        self.scale_ = scale\n",
    "        return self\n",
    "```\n",
    "\n",
    "Finally, we define our `predict` method,\n",
    "\n",
    "```python\n",
    "    def predict(self, X):\n",
    "\n",
    "        fut_mean, fut_std, fut_zscore = _get_fut_stats(X.squeeze(), self.window_width)\n",
    "        shift_expanded, scale_expanded = _expand_params(X.squeeze(), self.shift_, self.scale_)\n",
    "\n",
    "        fut_mean_corrected, fut_std_corrected = _correct_fut_stats(\n",
    "            fut_mean, fut_std, shift_expanded, scale_expanded\n",
    "        )\n",
    "\n",
    "        self.fut_stats_dict_ = {\n",
    "            \"meani\": fut_mean,\n",
    "            \"stdi\": fut_std,\n",
    "            \"meanf\": fut_mean_corrected,\n",
    "            \"stdf\": fut_std_corrected,\n",
    "        }\n",
    "\n",
    "        fut_corrected = (fut_zscore * fut_std_corrected) + fut_mean_corrected\n",
    "\n",
    "        return fut_corrected.to_frame(name)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skdownscale.pointwise_models import ZScoreRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a small dataset\n",
    "training = get_sample_data('wind-hist')\n",
    "target = get_sample_data('wind-obs')\n",
    "future = get_sample_data('wind-rcp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias correction using ZScoreRegresssor\n",
    "zscore = ZScoreRegressor()\n",
    "zscore.fit(training, target)\n",
    "fit_stats = zscore.fit_stats_dict_\n",
    "out = zscore.predict(future)\n",
    "predict_stats = zscore.predict_stats_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the datasets\n",
    "from utils import zscore_ds_plot\n",
    "\n",
    "zscore_ds_plot(training, target, future, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import zscore_correction_plot\n",
    "\n",
    "zscore_correction_plot(zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Automatic Parallelization\n",
    "\n",
    "In the examples above, we have performed downscaling on sample data sourced from individual points. In many downscaling workflows, however, users will want to apply pointwise methods at all points in their study domain. For this use case, scikit-downscale provides a high-level wrapper class: `PointWiseDownscaler`.\n",
    "\n",
    "In the example below, we'll use the `BCSDTemperature` model, along with the `PointWiseDownscaler` wrapper, to downscale daily maximum surface air temperature from CMIP6 for all point in a subset of the Pacific Norwest. We'll use a local [Dask Cluster](https://dask.org/) to distribute the computation among our available processors. Though not the point of this example, we also use [intake-esm](https://intake-esm.readthedocs.io/en/latest/) to access [CMIP6](https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6) data stored on [Google Cloud Storage](https://console.cloud.google.com/marketplace/details/noaa-public/cmip6).\n",
    "\n",
    "**Data:**\n",
    "  - Training / Prediction data: NASA-GISS-E2 historical data from CMIP6\n",
    "  - Targets: [GridMet](http://www.climatologylab.org/gridmet.html) daily maximum surface air temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "train_slice = slice('1980', '1982')  # train time range\n",
    "holdout_slice = slice('1990', '1991')  # prediction time range\n",
    "\n",
    "# bounding box of downscaling region\n",
    "lon_slice = slice(-124.8, -120.0) \n",
    "lat_slice = slice(50, 45)\n",
    "\n",
    "# chunk shape for dask execution (time must be contiguous, ie -1)\n",
    "chunks = {'lat': 10, 'lon': 10, 'time': -1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Start a Dask Cluster. Xarray and the `PointWiseDownscaler` will make use of this cluster when it comes time to load input data and train/predict downscaling models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Load our target data. \n",
    "\n",
    "Here we use xarray directly to load a collection of OpenDAP endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "fnames = [f'http://thredds.northwestknowledge.net:8080/thredds/dodsC/MET/tmmx/tmmx_{year}.nc'\n",
    "          for year in range(int(train_slice.start), int(train_slice.stop) + 1)]\n",
    "# open the data and cleanup a bit of metadata\n",
    "obs = xr.open_mfdataset(fnames, engine='pydap', concat_dim='day').rename({'day': 'time'}).drop('crs')\n",
    "\n",
    "obs_subset = obs['air_temperature'].sel(time=train_slice, lon=lon_slice, lat=lat_slice).resample(time='1d').mean().load(scheduler='threads').chunk(chunks)\n",
    "\n",
    "# display\n",
    "display(obs_subset)\n",
    "obs_subset.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Load our training/prediction data.\n",
    "\n",
    "Here we use `intake-esm` to access a single Xarray dataset from the Pangeo's Google Cloud CMIP6 data catalog. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "\n",
    "# search the cmip6 catalog\n",
    "col = intake.open_esm_datastore(\"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\")\n",
    "cat = col.search(experiment_id=['historical', 'ssp585'], table_id='day', variable_id='tasmax',\n",
    "                 grid_label='gn')\n",
    "\n",
    "# access the data and do some cleanup\n",
    "ds_model = cat['CMIP.NASA-GISS.GISS-E2-1-G.historical.day.gn'].to_dask().squeeze(drop=True).drop(['height', 'lat_bnds', 'lon_bnds', 'time_bnds'])\n",
    "ds_model.lon.values[ds_model.lon.values > 180] -= 360\n",
    "ds_model = ds_model.roll(lon=72, roll_coords=True)\n",
    "\n",
    "# regional subsets, ready for downscaling\n",
    "train_subset = ds_model['tasmax'].sel(time=train_slice).interp_like(obs_subset.isel(time=0, drop=True), method='linear')\n",
    "train_subset['time'] = train_subset.indexes['time'].to_datetimeindex()\n",
    "train_subset = train_subset.resample(time='1d').mean().load(scheduler='threads').chunk(chunks)\n",
    "\n",
    "holdout_subset = ds_model['tasmax'].sel(time=holdout_slice).interp_like(obs_subset.isel(time=0, drop=True), method='linear')\n",
    "holdout_subset['time'] = holdout_subset.indexes['time'].to_datetimeindex()\n",
    "holdout_subset = holdout_subset.resample(time='1d').mean().load(scheduler='threads').chunk(chunks)\n",
    "\n",
    "# display\n",
    "display(train_subset)\n",
    "train_subset.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4. Now that we have loaded our training and target data, we can move on to fit our BcsdTemperature model at each x/y point in our domain. This is where the `PointWiseDownscaler` comes in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skdownscale.pointwise_models import PointWiseDownscaler\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "model = PointWiseDownscaler(BcsdTemperature(return_anoms=False))\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5. We fit the `PointWiseDownscaler`, passing it data in Xarray data structures (our regional subsets from above). This opperation is lazy and return immediately. Under the hood, we can see that `PointWiseDownscaler._models` is an `Xarray.DataArray` of `BcsdTemperature` models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_subset, obs_subset)\n",
    "display(model, model._models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6. Finally, we can use our model to complete the downscaling workflow using the `predict` method along with our `holdout_subset` of CMIP6 data. We call the `.load()` method to eagerly compute the data. We end by plotting a map of downscaled data over our study area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(holdout_subset).load()\n",
    "display(predicted)\n",
    "predicted.isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Spatial Models\n",
    "Spatial models is a second class of downscaling methods that use information from the full study domain to form relationships between observations and ESM data. Scikit-downscale implements these models as as SpatialDownscaler. Beyond providing fit and predict methods that accept Xarray objects, the internal layout of these methods is intentionally unspecified. We are currently working on wrapping a few popular spatial downscaling models such as:\n",
    "\n",
    "- [MACA: Multivariate Adaptive Constructed Analogs](http://www.climatologylab.org/maca.html), Abatzoglou and Brown (2012)\n",
    "- [LOCA: Localized Constructed Analogs](http://loca.ucsd.edu/), Pierce, Cayan, and Thrasher (2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discussion\n",
    "\n",
    "### 3.1 Benchmark Applications\n",
    "Its likely that one of the reasons we haven’t seen strong consensus develop around particularl downscaling methodologies is the abscense of widely available benchamrk applications to test methods against eachother. While Scikit-downscale will not solve this problem on its own, we hope the ability to implemnt downscaling applications within a common framework will enable a more robust benchmarking inititive that previously possible.\n",
    "\n",
    "### 3.2 Call for Participation\n",
    "The Scikit-downscale effort is just getting started. With the recent release of [CMIP6](https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6), we expect a surge of interest in downscaled climate data. There are clear opportunities for involvement from climate impacts practicioneers, computer scientists with an interest in machine learning for climate applications, and climate scientists alike. Please reach out if you are interested in participating in any way.\n",
    "\n",
    "## References\n",
    "\n",
    "1. Abatzoglou, J. T. (2013), Development of gridded surface meteorological data for ecological applications and modelling. Int. J. Climatol., 33: 121–131.\n",
    "1. Abatzoglou J.T. and Brown T.J. A comparison of statistical downscaling methods suited for wildfire applications, International Journal of Climatology (2012), 32, 772-780\n",
    "1. Gutmann, E., Pruitt, T., Clark, M. P., Brekke, L., Arnold, J. R., Raff, D. A., and Rasmussen, R. M. ( 2014), An intercomparison of statistical downscaling methods used for water resource assessments in the United States, Water Resour. Res., 50, 7167– 7186, doi:10.1002/2014WR015559.\n",
    "1. Gutmann, E., J. Hamman, M. Clark, T. Eidhammer, A. Wood, J. Arnold, K. Nowak (in prep), Evaluating the effect of statistical downscaling methodological choices in a common framework. To be submitted to JGR-Atomspheres.\n",
    "1. Hamlet, A.F., Salathé, E.P., and Carrasco, P., 2010. Statistical downscaling techniques for global climate model simulations of temperature and precipitation with application to water resources planning studies. A report prepared by the Climate Impact Group for Columbia Basin Climate Change Scenario Project, University of Washington, Seattle, WA.\n",
    "http://www.hydro.washington.edu/2860/products/sites/r7climate/study_report/CBCCSP_chap4_gcm_draft_20100111.pdf\n",
    "1. Lanzante JR, KW Dixon, MJ Nath, CE Whitlock, and D Adams-Smith (2018): Some Pitfalls in Statistical Downscaling of Future Climate. Bulletin of the American Meteorological Society. DOI: 0.1175/BAMS-D-17-0046.1.\n",
    "1. Pierce, D. W., D. R. Cayan, and B. L. Thrasher, 2014: Statistical downscaling using Localized Constructed Analogs (LOCA). Journal of Hydrometeorology, volume 15, page 2558-2585\n",
    "1. Stoner, A., K. Hayhoe, and X. Yang (2012), An asynchronous regional regression model for statistical downscaling of daily climate variables, Int. J. Climatol., 33, 2473–2494, doi:10.1002/joc.3603.\n",
    "1. Wood, A., L. Leung, V. Sridhar, and D. Lettenmaier (2004), Hydrologic implications of dynamical and statistical approaches to downscaling climate model outputs, Clim. Change, 62, 189–216."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('pangeo': conda)",
   "language": "python",
   "name": "python36864bitpangeoconda84573a9ddafd4260af0faa8c0d62d45c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}